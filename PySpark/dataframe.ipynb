{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7aba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DiveInDataframes\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc84d22",
   "metadata": {},
   "source": [
    "### Why use manual schema?\n",
    "\n",
    "- When schema inference is expensive on large files.\n",
    "\n",
    "- To enforce correct types (prevent errors).\n",
    "\n",
    "- To ensure consistent schema across multiple reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad7cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- marks: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"marks\", DoubleType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "manual_df = spark.read.csv(\"students.csv\", header=True, schema=schema)\n",
    "manual_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68dd3637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----+\n",
      "| id| name|marks|city|\n",
      "+---+-----+-----+----+\n",
      "|  1|Aisha| NULL|  90|\n",
      "|  2|  Raj| NULL|  80|\n",
      "|  3| Neha| NULL|  85|\n",
      "+---+-----+-----+----+\n",
      "only showing top 3 rows\n",
      "None\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- marks: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------+------------------+-----+-----+-----------------+\n",
      "|summary|                id| name|marks|             city|\n",
      "+-------+------------------+-----+-----+-----------------+\n",
      "|  count|                 5|    5|    0|                5|\n",
      "|   mean|               3.0| NULL| NULL|             82.6|\n",
      "| stddev|1.5811388300841898| NULL| NULL|7.987490219086343|\n",
      "|    min|                 1|Aisha| NULL|               70|\n",
      "|    max|                 5|  Raj| NULL|               90|\n",
      "+-------+------------------+-----+-----+-----------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 16:59:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID, Name, Subject, Score\n",
      " Schema: id, name, marks, city\n",
      "Expected: marks but found: Subject\n",
      "CSV file: file:///home/developer/Workspace_Projects/Data_Engineer/PySpark/students.csv\n",
      "25/11/03 16:59:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID, Name, Subject, Score\n",
      " Schema: id, name, marks, city\n",
      "Expected: marks but found: Subject\n",
      "CSV file: file:///home/developer/Workspace_Projects/Data_Engineer/PySpark/students.csv\n"
     ]
    }
   ],
   "source": [
    "print(manual_df.show(3))\n",
    "print(manual_df.printSchema())\n",
    "print(manual_df.describe().show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "595cef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|  Name|Age|City|\n",
      "+------+---+----+\n",
      "|  John| 23|  NY|\n",
      "| Alice| 29|  LA|\n",
      "|Robert| 34|  SF|\n",
      "+------+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(\"John\", 23, \"NY\"),\n",
    "        (\"Alice\", 29, \"LA\"),\n",
    "        (\"Robert\", 34, \"SF\")]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"City\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc772076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|FullName|\n",
      "+--------+\n",
      "|    John|\n",
      "|   Alice|\n",
      "|  Robert|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"Name\"].alias(\"FullName\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f38f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|  Name|Age|City|\n",
      "+------+---+----+\n",
      "| Alice| 29|  LA|\n",
      "|Robert| 34|  SF|\n",
      "+------+---+----+\n",
      "\n",
      "+----+---+----+\n",
      "|Name|Age|City|\n",
      "+----+---+----+\n",
      "|John| 23|  NY|\n",
      "+----+---+----+\n",
      "\n",
      "+------+---+----+\n",
      "|  Name|Age|City|\n",
      "+------+---+----+\n",
      "| Alice| 29|  LA|\n",
      "|Robert| 34|  SF|\n",
      "+------+---+----+\n",
      "\n",
      "+-----+---+----+\n",
      "| Name|Age|City|\n",
      "+-----+---+----+\n",
      "| John| 23|  NY|\n",
      "|Alice| 29|  LA|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Age > 25).show()\n",
    "df.where(df.City == \"NY\").show()\n",
    "df.filter(df.City.isin(\"LA\", \"SF\")).show()\n",
    "df.filter(df.Age.between(20, 30)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2ea1480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+--------+\n",
      "|  Name|Age|City|Category|\n",
      "+------+---+----+--------+\n",
      "|  John| 23|  NY|   Young|\n",
      "| Alice| 29|  LA|   Adult|\n",
      "|Robert| 34|  SF|   Adult|\n",
      "+------+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df4 = df.withColumn(\"Category\", when(df.Age < 25, \"Young\").otherwise(\"Adult\"))\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3c77f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+\n",
      "|  Name|Age|    City|\n",
      "+------+---+--------+\n",
      "|  John| 23|New York|\n",
      "| Alice| 29|      LA|\n",
      "|Robert| 34|      SF|\n",
      "+------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.replace(\"NY\", \"New York\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9528d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Name='John', Age=23, City='NY'), Row(Name='Alice', Age=29, City='LA')]\n"
     ]
    }
   ],
   "source": [
    "rdd_from_df = df.rdd\n",
    "print(rdd_from_df.take(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a2b63e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------------+------+--------+\n",
      "|  Country|TotalStudents|           AvgAge|Oldest|Youngest|\n",
      "+---------+-------------+-----------------+------+--------+\n",
      "|    India|          155|40.66451612903226|    60|      21|\n",
      "|      USA|          153|40.22875816993464|    60|      21|\n",
      "|       UK|          154|41.16233766233766|    60|      21|\n",
      "|Australia|          138|40.88405797101449|    60|      21|\n",
      "+---------+-------------+-----------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max, count\n",
    "\n",
    "df = spark.read.parquet(\"modern_employee_data.parquet\", header=True, inferSchema=True)\n",
    "\n",
    "grouped_df = df.groupBy(\"Country\").agg(\n",
    "    count(\"*\").alias(\"TotalStudents\"),\n",
    "    avg(\"Age\").alias(\"AvgAge\"),\n",
    "    max(\"Age\").alias(\"Oldest\"),\n",
    "    min(\"Age\").alias(\"Youngest\")\n",
    ")\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "136ad392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "|employee_id|               name|gender|age|country|             state|department|salary|joining_date|experience_years|performance_score|               email|  phone_number|\n",
      "+-----------+-------------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "|          1|      Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|  326-034-8112|\n",
      "|          2|         Divij Raja| Other| 60|     UK|          Bhilwara| Marketing| 83808|  2016-05-25|               2|             4.57|lagan10@krishnan-...|   07045303968|\n",
      "|          3|        Ivana Divan|  Male| 29|  India|         Rajasthan| Marketing|160911|  2019-10-24|              11|             2.52|gargdarshit@hotma...|    1654209912|\n",
      "|          4|Miss Margaret Lucas| Other| 24|  India|       Maharashtra|        HR|244675|  2022-12-01|               7|             4.53|howarddale@yahoo....|+44121 4960026|\n",
      "|          5| Ms Kathleen Turner| Other| 34|     UK|          Hallstad|     Sales|114263|  2016-09-15|              16|             2.67|tmistry@gould-ali...|    0292018628|\n",
      "+-----------+-------------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e02602e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  Country|count|\n",
      "+---------+-----+\n",
      "|    India|  155|\n",
      "|      USA|  153|\n",
      "|       UK|  154|\n",
      "|Australia|  138|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2461a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|department|        Avg_Salary|\n",
      "+----------+------------------+\n",
      "|     Sales|140049.72727272726|\n",
      "|        HR| 141130.7882352941|\n",
      "|   Finance|153565.27472527474|\n",
      "|     Admin|141213.59259259258|\n",
      "| Marketing| 152743.7590361446|\n",
      "|        IT|149318.83529411766|\n",
      "|Operations|146407.54022988505|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df.groupBy(\"department\").agg(avg(\"salary\").alias(\"Avg_Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69668745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+---------+\n",
      "|  country|department|        avg_salary|emp_count|\n",
      "+---------+----------+------------------+---------+\n",
      "|      USA|   Finance|172351.62068965516|       29|\n",
      "|    India|        HR|145319.33333333334|       27|\n",
      "|Australia|     Admin|129202.76923076923|       26|\n",
      "|    India|     Sales|131644.76923076922|       26|\n",
      "|    India|        IT|143953.26923076922|       26|\n",
      "|       UK|Operations|148373.04166666666|       24|\n",
      "|       UK| Marketing|150033.16666666666|       24|\n",
      "|    India|   Finance|150180.08333333334|       24|\n",
      "|      USA|     Sales|145252.70833333334|       24|\n",
      "|       UK|        IT|137518.78260869565|       23|\n",
      "|       UK|     Sales|128299.82608695653|       23|\n",
      "|      USA| Marketing|156002.36363636365|       22|\n",
      "|       UK|     Admin|155517.86363636365|       22|\n",
      "|       UK|   Finance|146697.19047619047|       21|\n",
      "|Australia| Marketing|147776.09523809524|       21|\n",
      "|    India|Operations|166568.47619047618|       21|\n",
      "|Australia|Operations|          134692.0|       21|\n",
      "|      USA|        HR|144453.66666666666|       21|\n",
      "|      USA|Operations|135715.85714285713|       21|\n",
      "|Australia|        HR|         136356.15|       20|\n",
      "|      USA|        IT|154282.05555555556|       18|\n",
      "|      USA|     Admin|126315.61111111111|       18|\n",
      "|Australia|        IT|167183.72222222222|       18|\n",
      "|       UK|        HR|135990.88235294117|       17|\n",
      "+---------+----------+------------------+---------+\n",
      "only showing top 24 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df.groupBy(\"country\", \"department\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    count(\"*\").alias(\"emp_count\")\n",
    ").orderBy(desc(\"emp_count\")).show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3395f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "|department|avg_salary_with_bonus|\n",
      "+----------+---------------------+\n",
      "|     Sales|             154054.7|\n",
      "|        HR|            155243.87|\n",
      "|   Finance|             168921.8|\n",
      "|     Admin|            155334.95|\n",
      "| Marketing|            168018.13|\n",
      "|        IT|            164250.72|\n",
      "|Operations|            161048.29|\n",
      "+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.groupBy(\"department\").agg(\n",
    "    expr((\"round(avg(salary) * 1.1, 2)\")).alias(\"avg_salary_with_bonus\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3001407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|distinct_countries|approx_unique_emails|\n",
      "+------------------+--------------------+\n",
      "|                 4|                 589|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, approx_count_distinct\n",
    "\n",
    "df.agg(\n",
    "    countDistinct(\"Country\").alias(\"distinct_countries\"),\n",
    "    approx_count_distinct(\"email\").alias(\"approx_unique_emails\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "258bce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+\n",
      "|Country|department|total_salary|\n",
      "+-------+----------+------------+\n",
      "|    USA|     Sales|     3486065|\n",
      "|    USA|Operations|     2850033|\n",
      "|    USA| Marketing|     3432052|\n",
      "|    USA|        IT|     2777077|\n",
      "|    USA|        HR|     3033527|\n",
      "|    USA|   Finance|     4998197|\n",
      "|    USA|     Admin|     2273681|\n",
      "|    USA|      NULL|    22850632|\n",
      "|     UK|     Sales|     2950896|\n",
      "|     UK|Operations|     3560953|\n",
      "|     UK| Marketing|     3600796|\n",
      "|     UK|        IT|     3162932|\n",
      "|     UK|        HR|     2311845|\n",
      "|     UK|   Finance|     3080641|\n",
      "|     UK|     Admin|     3421393|\n",
      "|     UK|      NULL|    22089456|\n",
      "|  India|     Sales|     3422764|\n",
      "|  India|Operations|     3497938|\n",
      "|  India| Marketing|     2541586|\n",
      "|  India|        IT|     3742785|\n",
      "+-------+----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#  Goal: Subtotals and grand totals.\n",
    "\n",
    "df.rollup(\"Country\", \"department\").agg(\n",
    "    sum(\"salary\").alias(\"total_salary\")\n",
    ").orderBy(\"Country\", \"department\", ascending= False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8779a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+\n",
      "|  country|department|total_salary|\n",
      "+---------+----------+------------+\n",
      "|     NULL| Marketing|    12677732|\n",
      "|    India|     Admin|     2383955|\n",
      "|     NULL|   Finance|    13974440|\n",
      "|    India|     Sales|     3422764|\n",
      "|       UK|        HR|     2311845|\n",
      "|    India|        HR|     3923622|\n",
      "|       UK|Operations|     3560953|\n",
      "|    India|        IT|     3742785|\n",
      "|     NULL|     Admin|    11438301|\n",
      "|Australia| Marketing|     3103298|\n",
      "|      USA|Operations|     2850033|\n",
      "|       UK| Marketing|     3600796|\n",
      "|     NULL|     Sales|    12324376|\n",
      "|     NULL|Operations|    12737456|\n",
      "|       UK|        IT|     3162932|\n",
      "|    India|   Finance|     3604322|\n",
      "|     NULL|        HR|    11996117|\n",
      "|     NULL|      NULL|    87840523|\n",
      "|Australia|     Sales|     2464651|\n",
      "|      USA|   Finance|     4998197|\n",
      "+---------+----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.cube(\"country\", \"department\").agg(sum(\"salary\").alias(\"total_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5801c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------+------+---------+----------+------+\n",
      "|  country| Admin|Finance|    HR|    IT|Marketing|Operations| Sales|\n",
      "+---------+------+-------+------+------+---------+----------+------+\n",
      "|    India|243129| 246545|245801|247341|   235095|    243716|244013|\n",
      "|      USA|214719| 245062|233484|242102|   248181|    239365|245754|\n",
      "|       UK|249191| 244811|247480|243858|   248213|    247156|249185|\n",
      "|Australia|241577| 249375|243805|247517|   244619|    249484|244045|\n",
      "+---------+------+-------+------+------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df.groupBy(\"country\").pivot(\"department\").agg(max(\"salary\")).alias(\"Highest_Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f202fb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales → 140049.72727272726\n",
      "HR → 141130.7882352941\n",
      "Finance → 153565.27472527474\n",
      "Admin → 141213.59259259258\n",
      "Marketing → 152743.7590361446\n",
      "IT → 149318.83529411766\n",
      "Operations → 146407.54022988505\n"
     ]
    }
   ],
   "source": [
    "results = df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\")).collect()\n",
    "\n",
    "for row in results:\n",
    "    print(row[\"department\"], \"→\", row[\"avg_salary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5a1122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['country], ['country, unresolvedalias('avg('salary))]\n",
      "+- Relation [employee_id#1951,name#1952,gender#1953,age#1954,country#1955,state#1956,department#1957,salary#1958,joining_date#1959,experience_years#1960,performance_score#1961,email#1962,phone_number#1963] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "country: string, avg(salary): double\n",
      "Aggregate [country#1955], [country#1955, avg(salary#1958) AS avg(salary)#5898]\n",
      "+- Relation [employee_id#1951,name#1952,gender#1953,age#1954,country#1955,state#1956,department#1957,salary#1958,joining_date#1959,experience_years#1960,performance_score#1961,email#1962,phone_number#1963] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [country#1955], [country#1955, avg(salary#1958) AS avg(salary)#5898]\n",
      "+- Project [country#1955, salary#1958]\n",
      "   +- Relation [employee_id#1951,name#1952,gender#1953,age#1954,country#1955,state#1956,department#1957,salary#1958,joining_date#1959,experience_years#1960,performance_score#1961,email#1962,phone_number#1963] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[country#1955], functions=[avg(salary#1958)], output=[country#1955, avg(salary)#5898])\n",
      "   +- Exchange hashpartitioning(country#1955, 200), ENSURE_REQUIREMENTS, [plan_id=2967]\n",
      "      +- HashAggregate(keys=[country#1955], functions=[partial_avg(salary#1958)], output=[country#1955, sum#5901, count#5902L])\n",
      "         +- FileScan parquet [country#1955,salary#1958] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/modern_e..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<country:string,salary:int>\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.groupBy(\"country\").agg(avg(\"salary\")).explain(True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766c6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
