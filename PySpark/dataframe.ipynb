{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7aba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/04 11:01:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DiveInDataframes\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc84d22",
   "metadata": {},
   "source": [
    "### Why use manual schema?\n",
    "\n",
    "- When schema inference is expensive on large files.\n",
    "\n",
    "- To enforce correct types (prevent errors).\n",
    "\n",
    "- To ensure consistent schema across multiple reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad7cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- marks: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"marks\", DoubleType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "manual_df = spark.read.csv(\"students.csv\", header=True, schema=schema)\n",
    "manual_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68dd3637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----+\n",
      "| id| name|marks|city|\n",
      "+---+-----+-----+----+\n",
      "|  1|Aisha| NULL|  90|\n",
      "|  2|  Raj| NULL|  80|\n",
      "|  3| Neha| NULL|  85|\n",
      "+---+-----+-----+----+\n",
      "only showing top 3 rows\n",
      "None\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- marks: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------+------------------+-----+-----+-----------------+\n",
      "|summary|                id| name|marks|             city|\n",
      "+-------+------------------+-----+-----+-----------------+\n",
      "|  count|                 5|    5|    0|                5|\n",
      "|   mean|               3.0| NULL| NULL|             82.6|\n",
      "| stddev|1.5811388300841898| NULL| NULL|7.987490219086343|\n",
      "|    min|                 1|Aisha| NULL|               70|\n",
      "|    max|                 5|  Raj| NULL|               90|\n",
      "+-------+------------------+-----+-----+-----------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/03 16:59:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID, Name, Subject, Score\n",
      " Schema: id, name, marks, city\n",
      "Expected: marks but found: Subject\n",
      "CSV file: file:///home/developer/Workspace_Projects/Data_Engineer/PySpark/students.csv\n",
      "25/11/03 16:59:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ID, Name, Subject, Score\n",
      " Schema: id, name, marks, city\n",
      "Expected: marks but found: Subject\n",
      "CSV file: file:///home/developer/Workspace_Projects/Data_Engineer/PySpark/students.csv\n"
     ]
    }
   ],
   "source": [
    "print(manual_df.show(3))\n",
    "print(manual_df.printSchema())\n",
    "print(manual_df.describe().show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595cef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|  Name|Age|City|\n",
      "+------+---+----+\n",
      "|  John| 23|  NY|\n",
      "| Alice| 29|  LA|\n",
      "|Robert| 34|  SF|\n",
      "+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(\"John\", 23, \"NY\"),\n",
    "        (\"Alice\", 29, \"LA\"),\n",
    "        (\"Robert\", 34, \"SF\")]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"City\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc772076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|FullName|\n",
      "+--------+\n",
      "|    John|\n",
      "|   Alice|\n",
      "|  Robert|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"Name\"].alias(\"FullName\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f38f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|  Name|Age|City|\n",
      "+------+---+----+\n",
      "| Alice| 29|  LA|\n",
      "|Robert| 34|  SF|\n",
      "+------+---+----+\n",
      "\n",
      "+----+---+----+\n",
      "|Name|Age|City|\n",
      "+----+---+----+\n",
      "|John| 23|  NY|\n",
      "+----+---+----+\n",
      "\n",
      "+------+---+----+\n",
      "|  Name|Age|City|\n",
      "+------+---+----+\n",
      "| Alice| 29|  LA|\n",
      "|Robert| 34|  SF|\n",
      "+------+---+----+\n",
      "\n",
      "+-----+---+----+\n",
      "| Name|Age|City|\n",
      "+-----+---+----+\n",
      "| John| 23|  NY|\n",
      "|Alice| 29|  LA|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Age > 25).show()\n",
    "df.where(df.City == \"NY\").show()\n",
    "df.filter(df.City.isin(\"LA\", \"SF\")).show()\n",
    "df.filter(df.Age.between(20, 30)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2ea1480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+--------+\n",
      "|  Name|Age|City|Category|\n",
      "+------+---+----+--------+\n",
      "|  John| 23|  NY|   Young|\n",
      "| Alice| 29|  LA|   Adult|\n",
      "|Robert| 34|  SF|   Adult|\n",
      "+------+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df4 = df.withColumn(\"Category\", when(df.Age < 25, \"Young\").otherwise(\"Adult\"))\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3c77f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+\n",
      "|  Name|Age|    City|\n",
      "+------+---+--------+\n",
      "|  John| 23|New York|\n",
      "| Alice| 29|      LA|\n",
      "|Robert| 34|      SF|\n",
      "+------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.replace(\"NY\", \"New York\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9528d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Name='John', Age=23, City='NY'), Row(Name='Alice', Age=29, City='LA')]\n"
     ]
    }
   ],
   "source": [
    "rdd_from_df = df.rdd\n",
    "print(rdd_from_df.take(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2b63e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------------+------+--------+\n",
      "|  Country|TotalStudents|           AvgAge|Oldest|Youngest|\n",
      "+---------+-------------+-----------------+------+--------+\n",
      "|    India|          155|40.66451612903226|    60|      21|\n",
      "|      USA|          153|40.22875816993464|    60|      21|\n",
      "|       UK|          154|41.16233766233766|    60|      21|\n",
      "|Australia|          138|40.88405797101449|    60|      21|\n",
      "+---------+-------------+-----------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max, count\n",
    "\n",
    "df = spark.read.parquet(\"modern_employee_data.parquet\", header=True, inferSchema=True)\n",
    "\n",
    "grouped_df = df.groupBy(\"Country\").agg(\n",
    "    count(\"*\").alias(\"TotalStudents\"),\n",
    "    avg(\"Age\").alias(\"AvgAge\"),\n",
    "    max(\"Age\").alias(\"Oldest\"),\n",
    "    min(\"Age\").alias(\"Youngest\")\n",
    ")\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "136ad392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "|employee_id|               name|gender|age|country|             state|department|salary|joining_date|experience_years|performance_score|               email|  phone_number|\n",
      "+-----------+-------------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "|          1|      Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|  326-034-8112|\n",
      "|          2|         Divij Raja| Other| 60|     UK|          Bhilwara| Marketing| 83808|  2016-05-25|               2|             4.57|lagan10@krishnan-...|   07045303968|\n",
      "|          3|        Ivana Divan|  Male| 29|  India|         Rajasthan| Marketing|160911|  2019-10-24|              11|             2.52|gargdarshit@hotma...|    1654209912|\n",
      "|          4|Miss Margaret Lucas| Other| 24|  India|       Maharashtra|        HR|244675|  2022-12-01|               7|             4.53|howarddale@yahoo....|+44121 4960026|\n",
      "|          5| Ms Kathleen Turner| Other| 34|     UK|          Hallstad|     Sales|114263|  2016-09-15|              16|             2.67|tmistry@gould-ali...|    0292018628|\n",
      "+-----------+-------------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e02602e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  Country|count|\n",
      "+---------+-----+\n",
      "|    India|  155|\n",
      "|      USA|  153|\n",
      "|       UK|  154|\n",
      "|Australia|  138|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2461a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|department|        Avg_Salary|\n",
      "+----------+------------------+\n",
      "|     Sales|140049.72727272726|\n",
      "|        HR| 141130.7882352941|\n",
      "|   Finance|153565.27472527474|\n",
      "|     Admin|141213.59259259258|\n",
      "| Marketing| 152743.7590361446|\n",
      "|        IT|149318.83529411766|\n",
      "|Operations|146407.54022988505|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df.groupBy(\"department\").agg(avg(\"salary\").alias(\"Avg_Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69668745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+---------+\n",
      "|  country|department|        avg_salary|emp_count|\n",
      "+---------+----------+------------------+---------+\n",
      "|      USA|   Finance|172351.62068965516|       29|\n",
      "|    India|        HR|145319.33333333334|       27|\n",
      "|Australia|     Admin|129202.76923076923|       26|\n",
      "|    India|     Sales|131644.76923076922|       26|\n",
      "|    India|        IT|143953.26923076922|       26|\n",
      "|       UK|Operations|148373.04166666666|       24|\n",
      "|       UK| Marketing|150033.16666666666|       24|\n",
      "|    India|   Finance|150180.08333333334|       24|\n",
      "|      USA|     Sales|145252.70833333334|       24|\n",
      "|       UK|        IT|137518.78260869565|       23|\n",
      "|       UK|     Sales|128299.82608695653|       23|\n",
      "|      USA| Marketing|156002.36363636365|       22|\n",
      "|       UK|     Admin|155517.86363636365|       22|\n",
      "|       UK|   Finance|146697.19047619047|       21|\n",
      "|Australia| Marketing|147776.09523809524|       21|\n",
      "|    India|Operations|166568.47619047618|       21|\n",
      "|Australia|Operations|          134692.0|       21|\n",
      "|      USA|        HR|144453.66666666666|       21|\n",
      "|      USA|Operations|135715.85714285713|       21|\n",
      "|Australia|        HR|         136356.15|       20|\n",
      "|      USA|        IT|154282.05555555556|       18|\n",
      "|      USA|     Admin|126315.61111111111|       18|\n",
      "|Australia|        IT|167183.72222222222|       18|\n",
      "|       UK|        HR|135990.88235294117|       17|\n",
      "+---------+----------+------------------+---------+\n",
      "only showing top 24 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df.groupBy(\"country\", \"department\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    count(\"*\").alias(\"emp_count\")\n",
    ").orderBy(desc(\"emp_count\")).show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3395f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "|department|avg_salary_with_bonus|\n",
      "+----------+---------------------+\n",
      "|     Sales|             154054.7|\n",
      "|        HR|            155243.87|\n",
      "|   Finance|             168921.8|\n",
      "|     Admin|            155334.95|\n",
      "| Marketing|            168018.13|\n",
      "|        IT|            164250.72|\n",
      "|Operations|            161048.29|\n",
      "+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.groupBy(\"department\").agg(\n",
    "    expr((\"round(avg(salary) * 1.1, 2)\")).alias(\"avg_salary_with_bonus\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3001407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|distinct_countries|approx_unique_emails|\n",
      "+------------------+--------------------+\n",
      "|                 4|                 589|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, approx_count_distinct\n",
    "\n",
    "df.agg(\n",
    "    countDistinct(\"Country\").alias(\"distinct_countries\"),\n",
    "    approx_count_distinct(\"email\").alias(\"approx_unique_emails\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "258bce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+\n",
      "|Country|department|total_salary|\n",
      "+-------+----------+------------+\n",
      "|    USA|     Sales|     3486065|\n",
      "|    USA|Operations|     2850033|\n",
      "|    USA| Marketing|     3432052|\n",
      "|    USA|        IT|     2777077|\n",
      "|    USA|        HR|     3033527|\n",
      "|    USA|   Finance|     4998197|\n",
      "|    USA|     Admin|     2273681|\n",
      "|    USA|      NULL|    22850632|\n",
      "|     UK|     Sales|     2950896|\n",
      "|     UK|Operations|     3560953|\n",
      "|     UK| Marketing|     3600796|\n",
      "|     UK|        IT|     3162932|\n",
      "|     UK|        HR|     2311845|\n",
      "|     UK|   Finance|     3080641|\n",
      "|     UK|     Admin|     3421393|\n",
      "|     UK|      NULL|    22089456|\n",
      "|  India|     Sales|     3422764|\n",
      "|  India|Operations|     3497938|\n",
      "|  India| Marketing|     2541586|\n",
      "|  India|        IT|     3742785|\n",
      "+-------+----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#  Goal: Subtotals and grand totals.\n",
    "\n",
    "df.rollup(\"Country\", \"department\").agg(\n",
    "    sum(\"salary\").alias(\"total_salary\")\n",
    ").orderBy(\"Country\", \"department\", ascending= False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8779a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+\n",
      "|  country|department|total_salary|\n",
      "+---------+----------+------------+\n",
      "|     NULL| Marketing|    12677732|\n",
      "|    India|     Admin|     2383955|\n",
      "|     NULL|   Finance|    13974440|\n",
      "|    India|     Sales|     3422764|\n",
      "|       UK|        HR|     2311845|\n",
      "|    India|        HR|     3923622|\n",
      "|       UK|Operations|     3560953|\n",
      "|    India|        IT|     3742785|\n",
      "|     NULL|     Admin|    11438301|\n",
      "|Australia| Marketing|     3103298|\n",
      "|      USA|Operations|     2850033|\n",
      "|       UK| Marketing|     3600796|\n",
      "|     NULL|     Sales|    12324376|\n",
      "|     NULL|Operations|    12737456|\n",
      "|       UK|        IT|     3162932|\n",
      "|    India|   Finance|     3604322|\n",
      "|     NULL|        HR|    11996117|\n",
      "|     NULL|      NULL|    87840523|\n",
      "|Australia|     Sales|     2464651|\n",
      "|      USA|   Finance|     4998197|\n",
      "+---------+----------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.cube(\"country\", \"department\").agg(sum(\"salary\").alias(\"total_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5801c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------+------+---------+----------+------+\n",
      "|  country| Admin|Finance|    HR|    IT|Marketing|Operations| Sales|\n",
      "+---------+------+-------+------+------+---------+----------+------+\n",
      "|    India|243129| 246545|245801|247341|   235095|    243716|244013|\n",
      "|      USA|214719| 245062|233484|242102|   248181|    239365|245754|\n",
      "|       UK|249191| 244811|247480|243858|   248213|    247156|249185|\n",
      "|Australia|241577| 249375|243805|247517|   244619|    249484|244045|\n",
      "+---------+------+-------+------+------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df.groupBy(\"country\").pivot(\"department\").agg(max(\"salary\")).alias(\"Highest_Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f202fb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales → 140049.72727272726\n",
      "HR → 141130.7882352941\n",
      "Finance → 153565.27472527474\n",
      "Admin → 141213.59259259258\n",
      "Marketing → 152743.7590361446\n",
      "IT → 149318.83529411766\n",
      "Operations → 146407.54022988505\n"
     ]
    }
   ],
   "source": [
    "results = df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\")).collect()\n",
    "\n",
    "for row in results:\n",
    "    print(row[\"department\"], \"→\", row[\"avg_salary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5a1122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['country], ['country, unresolvedalias('avg('salary))]\n",
      "+- Relation [employee_id#1951,name#1952,gender#1953,age#1954,country#1955,state#1956,department#1957,salary#1958,joining_date#1959,experience_years#1960,performance_score#1961,email#1962,phone_number#1963] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "country: string, avg(salary): double\n",
      "Aggregate [country#1955], [country#1955, avg(salary#1958) AS avg(salary)#5898]\n",
      "+- Relation [employee_id#1951,name#1952,gender#1953,age#1954,country#1955,state#1956,department#1957,salary#1958,joining_date#1959,experience_years#1960,performance_score#1961,email#1962,phone_number#1963] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [country#1955], [country#1955, avg(salary#1958) AS avg(salary)#5898]\n",
      "+- Project [country#1955, salary#1958]\n",
      "   +- Relation [employee_id#1951,name#1952,gender#1953,age#1954,country#1955,state#1956,department#1957,salary#1958,joining_date#1959,experience_years#1960,performance_score#1961,email#1962,phone_number#1963] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[country#1955], functions=[avg(salary#1958)], output=[country#1955, avg(salary)#5898])\n",
      "   +- Exchange hashpartitioning(country#1955, 200), ENSURE_REQUIREMENTS, [plan_id=2967]\n",
      "      +- HashAggregate(keys=[country#1955], functions=[partial_avg(salary#1958)], output=[country#1955, sum#5901, count#5902L])\n",
      "         +- FileScan parquet [country#1955,salary#1958] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/modern_e..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<country:string,salary:int>\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.groupBy(\"country\").agg(avg(\"salary\")).explain(True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9766c6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) ColumnarToRow +- FileScan parquet [employee_id#17,name#\n",
      "18,gender#19,age#20,country#21,state#22,department#23,salary\n",
      "#24,joining_date#25,experience_years#26,performance_score#27\n",
      ",email#28,phone_number#29] Batched: true, DataFilters: [],\n",
      "Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/\n",
      "home/developer/Workspace_Projects/Data_Engineer/PySpark/mode\n",
      "rn_e..., PartitionFilters: [], PushedFilters: [],\n",
      "ReadSchema: struct<employee_id:int,name:string,gender:string\n",
      ",age:int,country:string,state:string,department:s...\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "plan_string = df._jdf.queryExecution().executedPlan().toString()\n",
    "\n",
    "# Wrap the string to a specific width (e.g., 80 characters)\n",
    "wrapped_plan = textwrap.fill(plan_string, width=60)\n",
    "\n",
    "print(wrapped_plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c44b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- joining_date: date (nullable = true)\n",
      " |-- experience_years: integer (nullable = true)\n",
      " |-- performance_score: double (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      "\n",
      "+----------+------+-------------+\n",
      "|Department|Budget|      Manager|\n",
      "+----------+------+-------------+\n",
      "| Marketing|500000|  Sarah Miles|\n",
      "|        HR|200000|  John Carter|\n",
      "|        IT|700000|   David Wong|\n",
      "|   Finance|400000| Priya Sharma|\n",
      "|     Sales|350000|  Luke Martin|\n",
      "|Operations|450000|Emma Thompson|\n",
      "|     Admin|250000| Chris Parker|\n",
      "+----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate()\n",
    "\n",
    "emp_df = spark.read.parquet(\"modern_employee_data.parquet\")\n",
    "dept_df = spark.read.csv(\"department_budget.csv\", header=True, inferSchema=True)\n",
    "\n",
    "emp_df.printSchema()\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a6a9651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----------+------+\n",
      "|               name|salary|department|budget|\n",
      "+-------------------+------+----------+------+\n",
      "|      Katrina Riley| 97018| Marketing|500000|\n",
      "|         Divij Raja| 83808| Marketing|500000|\n",
      "|        Ivana Divan|160911| Marketing|500000|\n",
      "|Miss Margaret Lucas|244675|        HR|200000|\n",
      "| Ms Kathleen Turner|114263|     Sales|350000|\n",
      "| Divyansh Sabharwal|194621|        IT|700000|\n",
      "|         Levi Mason|153641| Marketing|500000|\n",
      "|    Dr Terry Knight|246545|   Finance|400000|\n",
      "| Brittney Daugherty|202992|Operations|450000|\n",
      "|       Taylor Smith| 58293|        IT|700000|\n",
      "+-------------------+------+----------+------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "inner_join = emp_df.join(dept_df, \"department\", \"inner\")\n",
    "inner_join.select(\"name\", \"salary\", \"department\", \"budget\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42586de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------+\n",
      "|               name|department|budget|\n",
      "+-------------------+----------+------+\n",
      "|      Katrina Riley| Marketing|500000|\n",
      "|         Divij Raja| Marketing|500000|\n",
      "|        Ivana Divan| Marketing|500000|\n",
      "|Miss Margaret Lucas|        HR|200000|\n",
      "| Ms Kathleen Turner|     Sales|350000|\n",
      "| Divyansh Sabharwal|        IT|700000|\n",
      "|         Levi Mason| Marketing|500000|\n",
      "|    Dr Terry Knight|   Finance|400000|\n",
      "| Brittney Daugherty|Operations|450000|\n",
      "|       Taylor Smith|        IT|700000|\n",
      "+-------------------+----------+------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "left_join = emp_df.join(dept_df, \"department\", \"left\")\n",
    "left_join.select(\"name\", \"department\", \"budget\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b3f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------------+------+---+-------+------------------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "|department|employee_id|               name|gender|age|country|             state|salary|joining_date|experience_years|performance_score|               email|  phone_number|\n",
      "+----------+-----------+-------------------+------+---+-------+------------------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "| Marketing|          1|      Katrina Riley|  Male| 51|     UK|South Allisonmouth| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|  326-034-8112|\n",
      "| Marketing|          2|         Divij Raja| Other| 60|     UK|          Bhilwara| 83808|  2016-05-25|               2|             4.57|lagan10@krishnan-...|   07045303968|\n",
      "| Marketing|          3|        Ivana Divan|  Male| 29|  India|         Rajasthan|160911|  2019-10-24|              11|             2.52|gargdarshit@hotma...|    1654209912|\n",
      "|        HR|          4|Miss Margaret Lucas| Other| 24|  India|       Maharashtra|244675|  2022-12-01|               7|             4.53|howarddale@yahoo....|+44121 4960026|\n",
      "|     Sales|          5| Ms Kathleen Turner| Other| 34|     UK|          Hallstad|114263|  2016-09-15|              16|             2.67|tmistry@gould-ali...|    0292018628|\n",
      "|        IT|          6| Divyansh Sabharwal|  Male| 35|  India|         Karnataka|194621|  2016-08-17|              18|             4.71|bobalvedika@yahoo...|    6420526167|\n",
      "+----------+-----------+-------------------+------+---+-------+------------------+------+------------+----------------+-----------------+--------------------+--------------+\n",
      "only showing top 6 rows\n"
     ]
    }
   ],
   "source": [
    "# Only keeps employees whose department exists in the dept file\n",
    "semi_join = emp_df.join(dept_df, \"department\", \"left_semi\")\n",
    "semi_join.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfc21a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----+------+---+-------+-----+------+------------+----------------+-----------------+-----+------------+\n",
      "|department|employee_id|name|gender|age|country|state|salary|joining_date|experience_years|performance_score|email|phone_number|\n",
      "+----------+-----------+----+------+---+-------+-----+------+------------+----------------+-----------------+-----+------------+\n",
      "+----------+-----------+----+------+---+-------+-----+------+------------+----------------+-----------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anti_join = emp_df.join(dept_df, \"department\", \"left_anti\")\n",
    "anti_join.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54086285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+------------+----------+------+-------------+\n",
      "|employee_id|         name|gender|age|country|             state|department|salary|joining_date|experience_years|performance_score|               email|phone_number|Department|Budget|      Manager|\n",
      "+-----------+-------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+------------+----------+------+-------------+\n",
      "|          1|Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|326-034-8112| Marketing|500000|  Sarah Miles|\n",
      "|          1|Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|326-034-8112|        HR|200000|  John Carter|\n",
      "|          1|Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|326-034-8112|        IT|700000|   David Wong|\n",
      "|          1|Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|326-034-8112|   Finance|400000| Priya Sharma|\n",
      "|          1|Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|326-034-8112|     Sales|350000|  Luke Martin|\n",
      "|          1|Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|326-034-8112|Operations|450000|Emma Thompson|\n",
      "|          1|Katrina Riley|  Male| 51|     UK|South Allisonmouth| Marketing| 97018|  2020-04-30|               5|             3.88|   becky87@gmail.com|326-034-8112|     Admin|250000| Chris Parker|\n",
      "|          2|   Divij Raja| Other| 60|     UK|          Bhilwara| Marketing| 83808|  2016-05-25|               2|             4.57|lagan10@krishnan-...| 07045303968| Marketing|500000|  Sarah Miles|\n",
      "|          2|   Divij Raja| Other| 60|     UK|          Bhilwara| Marketing| 83808|  2016-05-25|               2|             4.57|lagan10@krishnan-...| 07045303968|        HR|200000|  John Carter|\n",
      "|          2|   Divij Raja| Other| 60|     UK|          Bhilwara| Marketing| 83808|  2016-05-25|               2|             4.57|lagan10@krishnan-...| 07045303968|        IT|700000|   David Wong|\n",
      "+-----------+-------------+------+---+-------+------------------+----------+------+------------+----------------+-----------------+--------------------+------------+----------+------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "cross_join = emp_df.crossJoin(dept_df)\n",
    "cross_join.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4192477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+----------------+\n",
      "|department|total_salary|  budget|budget_remaining|\n",
      "+----------+------------+--------+----------------+\n",
      "|     Sales|    12324376|35000000|        22675624|\n",
      "|        HR|    11996117|20000000|         8003883|\n",
      "|   Finance|    13974440|40000000|        26025560|\n",
      "|     Admin|    11438301|25000000|        13561699|\n",
      "| Marketing|    12677732|50000000|        37322268|\n",
      "|        IT|    12692101|70000000|        57307899|\n",
      "|Operations|    12737456|45000000|        32262544|\n",
      "+----------+------------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "\n",
    "combined_df = emp_df.join(dept_df, \"department\", \"inner\")\n",
    "\n",
    "dept_analysis = combined_df.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        F.sum(\"salary\").alias(\"total_salary\"),\n",
    "        F.first(\"budget\").alias(\"budget\")\n",
    "    ) \n",
    "\n",
    "dept_analysis = dept_analysis.withColumn(\n",
    "    \"budget_remaining\", F.col(\"budget\") - F.col(\"total_salary\")\n",
    ")\n",
    "\n",
    "dept_analysis.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4966405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank, dense_rank, row_number, avg, sum, lead, lag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71194fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+------+----+----------+----------+\n",
      "|              name|department|salary|rank|dense_rank|row_number|\n",
      "+------------------+----------+------+----+----------+----------+\n",
      "|      Nishith Kade|     Admin|249191|   1|         1|         1|\n",
      "|   Rebecca Collins|     Admin|244091|   2|         2|         2|\n",
      "|     Biju Aggarwal|     Admin|243782|   3|         3|         3|\n",
      "|    Kayleigh Patel|     Admin|243129|   4|         4|         4|\n",
      "|        Vidur Jani|     Admin|241577|   5|         5|         5|\n",
      "|      William Ware|     Admin|240562|   6|         6|         6|\n",
      "|    Pihu Zachariah|     Admin|236161|   7|         7|         7|\n",
      "|    Stephen Howard|     Admin|235240|   8|         8|         8|\n",
      "|      Paul Jenkins|     Admin|235228|   9|         9|         9|\n",
      "|     Hollie Wilson|     Admin|228204|  10|        10|        10|\n",
      "| Robert Richardson|     Admin|222452|  11|        11|        11|\n",
      "|      Seher Chahal|     Admin|218583|  12|        12|        12|\n",
      "| Bethan Williamson|     Admin|214719|  13|        13|        13|\n",
      "|      Dominique Le|     Admin|209808|  14|        14|        14|\n",
      "|       Roger Walls|     Admin|207615|  15|        15|        15|\n",
      "|  Alexander Rogers|     Admin|206590|  16|        16|        16|\n",
      "|Conor Baker-Taylor|     Admin|203793|  17|        17|        17|\n",
      "|  Nirvaan Bhardwaj|     Admin|203204|  18|        18|        18|\n",
      "|      Kabir Dhawan|     Admin|197825|  19|        19|        19|\n",
      "|    Lawrence Smith|     Admin|197146|  20|        20|        20|\n",
      "+------------------+----------+------+----+----------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "salary_range = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "ranked_df = emp_df.withColumn(\"rank\", rank().over(salary_range))\\\n",
    "                  .withColumn(\"dense_rank\", dense_rank().over(salary_range))\\\n",
    "                  .withColumn(\"row_number\", row_number().over(salary_range))\n",
    "\n",
    "ranked_df.select(\"name\", \"department\", \"salary\", \"rank\", \"dense_rank\", \"row_number\").show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
