{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8dec800",
   "metadata": {},
   "source": [
    "#### Broadcast Hash Join (BHJ)\n",
    "\n",
    "- When: One dataset is very small (fits in each executor‚Äôs memory, typically <10MB by default).\n",
    "\n",
    "- How: Spark broadcasts (sends a full copy of) the small dataset to all executors.\n",
    "\n",
    "- Why: Avoids shuffling the big dataset ‚Äî fastest possible join when one side is small.\n",
    "\n",
    "- Cost: Broadcast communication overhead, but no shuffle.\n",
    "\n",
    "- Hint: broadcast(df) or df.hint(\"broadcast\").\n",
    "\n",
    "When you join two datasets in Spark (say orders and customers), Spark must shuffle data between executors so that rows with the same join key land on the same machine.\n",
    "\n",
    "This shuffle is expensive ‚Äî it involves:\n",
    "\n",
    "-Disk I/O\n",
    "\n",
    "-Network transfer\n",
    "\n",
    "-Serialization/deserialization\n",
    "\n",
    "-Memory overhead\n",
    "\n",
    "üí° So if one dataset is small enough, we can avoid shuffle altogether by sending that small dataset to every executor.\n",
    "This is called a Broadcast Join (or Map-side Join)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed52231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+---+\n",
      "|cust_id| name|region|age|\n",
      "+-------+-----+------+---+\n",
      "|      1|Alice| North| 28|\n",
      "|      2|  Bob| South| 35|\n",
      "|      3|Carol|  East| 40|\n",
      "|      4|David|  West| 23|\n",
      "|      5|  Eva| South| 31|\n",
      "+-------+-----+------+---+\n",
      "only showing top 5 rows\n",
      "+--------+-------+----------+------+\n",
      "|order_id|cust_id|order_date|amount|\n",
      "+--------+-------+----------+------+\n",
      "|     101|      1|2024-01-01|   250|\n",
      "|     102|      2|2024-01-03|   300|\n",
      "|     103|      2|2024-01-05|   150|\n",
      "|     104|      3|2024-02-01|   500|\n",
      "|     105|      5|2024-02-12|   400|\n",
      "+--------+-------+----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"JoinStrategiesDemo\").getOrCreate()\n",
    "\n",
    "customers = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
    "orders = spark.read.csv(\"order.csv\", header=True, inferSchema=True)\n",
    "\n",
    "customers.show(5)\n",
    "orders.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8fc9a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [cust_id#117], [cust_id#95], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(cust_id#117)\n",
      "   :  +- FileScan csv [order_id#116,cust_id#117,order_date#118,amount#119] Batched: false, DataFilters: [isnotnull(cust_id#117)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/order.csv], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<order_id:int,cust_id:int,order_date:date,amount:int>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=145]\n",
      "      +- Filter isnotnull(cust_id#95)\n",
      "         +- FileScan csv [cust_id#95,name#96,region#97,age#98] Batched: false, DataFilters: [isnotnull(cust_id#95)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/customer..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:int,name:string,region:string,age:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "bhj_df = orders.join(broadcast(customers), orders.cust_id == customers.cust_id, \"inner\")\n",
    "bhj_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fac3c",
   "metadata": {},
   "source": [
    "#### Shuffle Hash Join (SHJ)\n",
    "\n",
    "- When: Both sides are moderate in size but not sorted; each must be shuffled so rows with same join key end up on same partition.\n",
    "\n",
    "- Spark builds hash tables on one side (usually smaller).\n",
    "\n",
    "- Cost: Both sides shuffle, but uses in-memory hash map to probe.\n",
    "\n",
    "- Requirements: Join key must be equi-join (using ==)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c47c37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [cust_id#117, order_id#116, order_date#118, amount#119, name#96, region#97, age#98]\n",
      "   +- SortMergeJoin [cust_id#117], [cust_id#95], Inner\n",
      "      :- Sort [cust_id#117 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(cust_id#117, 200), ENSURE_REQUIREMENTS, [plan_id=470]\n",
      "      :     +- Filter isnotnull(cust_id#117)\n",
      "      :        +- FileScan csv [order_id#116,cust_id#117,order_date#118,amount#119] Batched: false, DataFilters: [isnotnull(cust_id#117)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/order.csv], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<order_id:int,cust_id:int,order_date:date,amount:int>\n",
      "      +- Sort [cust_id#95 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(cust_id#95, 200), ENSURE_REQUIREMENTS, [plan_id=471]\n",
      "            +- Filter isnotnull(cust_id#95)\n",
      "               +- FileScan csv [cust_id#95,name#96,region#97,age#98] Batched: false, DataFilters: [isnotnull(cust_id#95)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/customer..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:int,name:string,region:string,age:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "shj_df = orders.join(customers, \"cust_id\")\n",
    "shj_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60ef9c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [cust_id#117, order_id#116, order_date#118, amount#119, name#96, region#97, age#98]\n",
      "   +- ShuffledHashJoin [cust_id#117], [cust_id#95], Inner, BuildLeft\n",
      "      :- Exchange hashpartitioning(cust_id#117, 200), ENSURE_REQUIREMENTS, [plan_id=871]\n",
      "      :  +- Filter isnotnull(cust_id#117)\n",
      "      :     +- FileScan csv [order_id#116,cust_id#117,order_date#118,amount#119] Batched: false, DataFilters: [isnotnull(cust_id#117)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/order.csv], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<order_id:int,cust_id:int,order_date:date,amount:int>\n",
      "      +- Exchange hashpartitioning(cust_id#95, 200), ENSURE_REQUIREMENTS, [plan_id=872]\n",
      "         +- Filter isnotnull(cust_id#95)\n",
      "            +- FileScan csv [cust_id#95,name#96,region#97,age#98] Batched: false, DataFilters: [isnotnull(cust_id#95)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/customer..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:int,name:string,region:string,age:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.hint(\"shuffle_hash\").join(customers, \"cust_id\").explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858aa64",
   "metadata": {},
   "source": [
    "#### Sort Merge Join (SMJ)\n",
    "\n",
    "- When: Large datasets; Spark sorts both sides on the join key and then merges them (like merging two sorted arrays).\n",
    "\n",
    "- How: Requires both sides to be hash-partitioned on join key, then sorted.\n",
    "\n",
    "- Cost: Heavy CPU on sorting + shuffling both sides.\n",
    "\n",
    "- Benefit: Very scalable and stable; used for large fact‚Äìfact joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78318eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [cust_id])\n",
      ":- ResolvedHint (strategy=merge)\n",
      ":  +- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "+- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: int, order_id: int, order_date: date, amount: int, name: string, region: string, age: int\n",
      "Project [cust_id#117, order_id#116, order_date#118, amount#119, name#96, region#97, age#98]\n",
      "+- Join Inner, (cust_id#117 = cust_id#95)\n",
      "   :- ResolvedHint (strategy=merge)\n",
      "   :  +- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "   +- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [cust_id#117, order_id#116, order_date#118, amount#119, name#96, region#97, age#98]\n",
      "+- Join Inner, (cust_id#117 = cust_id#95), leftHint=(strategy=merge)\n",
      "   :- Filter isnotnull(cust_id#117)\n",
      "   :  +- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "   +- Filter isnotnull(cust_id#95)\n",
      "      +- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [cust_id#117, order_id#116, order_date#118, amount#119, name#96, region#97, age#98]\n",
      "   +- SortMergeJoin [cust_id#117], [cust_id#95], Inner\n",
      "      :- Sort [cust_id#117 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(cust_id#117, 200), ENSURE_REQUIREMENTS, [plan_id=838]\n",
      "      :     +- Filter isnotnull(cust_id#117)\n",
      "      :        +- FileScan csv [order_id#116,cust_id#117,order_date#118,amount#119] Batched: false, DataFilters: [isnotnull(cust_id#117)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/order.csv], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<order_id:int,cust_id:int,order_date:date,amount:int>\n",
      "      +- Sort [cust_id#95 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(cust_id#95, 200), ENSURE_REQUIREMENTS, [plan_id=839]\n",
      "            +- Filter isnotnull(cust_id#95)\n",
      "               +- FileScan csv [cust_id#95,name#96,region#97,age#98] Batched: false, DataFilters: [isnotnull(cust_id#95)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/customer..., PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<cust_id:int,name:string,region:string,age:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) \n",
    "smj_df = orders.hint(\"merge\").join(customers, \"cust_id\")\n",
    "smj_df.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c637cf",
   "metadata": {},
   "source": [
    "#### Shuffle-and-Replicate Nested Loop Join (a.k.a. Cartesian Join)\n",
    "\n",
    "\n",
    "When: Cross join between two large datasets (no join condition at all).\n",
    "\n",
    "How: Spark replicates all partitions of one side to every partition of the other.\n",
    "\n",
    "Cost: Extremely expensive ‚Äî can produce billions of combinations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90002dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Cross\n",
      ":- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "+- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: int, name: string, region: string, age: int, order_id: int, cust_id: int, order_date: date, amount: int\n",
      "Join Cross\n",
      ":- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "+- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Cross\n",
      ":- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "+- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "CartesianProduct\n",
      ":- FileScan csv [cust_id#95,name#96,region#97,age#98] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/customer..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cust_id:int,name:string,region:string,age:int>\n",
      "+- FileScan csv [order_id#116,cust_id#117,order_date#118,amount#119] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/order.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:int,cust_id:int,order_date:date,amount:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_df = customers.crossJoin(orders)\n",
    "cross_df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe71855c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, ((cust_id#95 = cust_id#117) AND (age#98 > 25))\n",
      ":- SubqueryAlias c\n",
      ":  +- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- SubqueryAlias o\n",
      "      +- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "cust_id: int, name: string, region: string, age: int, order_id: int, cust_id: int, order_date: date, amount: int\n",
      "Join Inner, ((cust_id#95 = cust_id#117) AND (age#98 > 25))\n",
      ":- SubqueryAlias c\n",
      ":  +- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- SubqueryAlias o\n",
      "      +- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (cust_id#95 = cust_id#117), rightHint=(strategy=broadcast)\n",
      ":- Filter ((isnotnull(age#98) AND (age#98 > 25)) AND isnotnull(cust_id#95))\n",
      ":  +- Relation [cust_id#95,name#96,region#97,age#98] csv\n",
      "+- Filter isnotnull(cust_id#117)\n",
      "   +- Relation [order_id#116,cust_id#117,order_date#118,amount#119] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [cust_id#95], [cust_id#117], Inner, BuildRight, false\n",
      "   :- Filter ((isnotnull(age#98) AND (age#98 > 25)) AND isnotnull(cust_id#95))\n",
      "   :  +- FileScan csv [cust_id#95,name#96,region#97,age#98] Batched: false, DataFilters: [isnotnull(age#98), (age#98 > 25), isnotnull(cust_id#95)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/customer..., PartitionFilters: [], PushedFilters: [IsNotNull(age), GreaterThan(age,25), IsNotNull(cust_id)], ReadSchema: struct<cust_id:int,name:string,region:string,age:int>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, false] as bigint)),false), [plan_id=1006]\n",
      "      +- Filter isnotnull(cust_id#117)\n",
      "         +- FileScan csv [order_id#116,cust_id#117,order_date#118,amount#119] Batched: false, DataFilters: [isnotnull(cust_id#117)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/developer/Workspace_Projects/Data_Engineer/PySpark/order.csv], PartitionFilters: [], PushedFilters: [IsNotNull(cust_id)], ReadSchema: struct<order_id:int,cust_id:int,order_date:date,amount:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "bnlj_df = customers.alias(\"c\").join(\n",
    "    broadcast(orders.alias(\"o\")),\n",
    "    (col(\"c.cust_id\") == col(\"o.cust_id\")) & (col(\"c.age\") > 25),\n",
    "    \"inner\"\n",
    ")\n",
    "bnlj_df.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37848343",
   "metadata": {},
   "source": [
    "`                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                  ‚îÇ Spark Join   ‚îÇ\n",
    "                  ‚îÇ Strategies   ‚îÇ\n",
    "                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ                   ‚îÇ                     ‚îÇ\n",
    " Broadcast            Shuffle              Nested Loop\n",
    "     ‚îÇ                   ‚îÇ                     ‚îÇ\n",
    "     ‚ñº                   ‚ñº                     ‚ñº\n",
    "Broadcast Hash     Shuffle Hash         Broadcast NL\n",
    " (small + big)      (medium size)       (small + non-equi)\n",
    "                       ‚ñº\n",
    "                   Sort Merge\n",
    "                  (large + equi)\n",
    "                       ‚ñº\n",
    "                   Shuffle-Replicate NL\n",
    "                (large + non-equi)\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
