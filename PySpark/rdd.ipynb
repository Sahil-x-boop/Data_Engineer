{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "706c1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Python Spark create RDD example\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8a9bbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(2) PythonRDD[128] at collect at /tmp/ipykernel_87469/3632553374.py:11 []\\n |  employees.csv MapPartitionsRDD[126] at textFile at <unknown>:0 []\\n |  employees.csv HadoopRDD[125] at textFile at <unknown>:0 []'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'Alice', 'Engineering', 95000, 29, 'Boston'),\n",
       " (2, 'Bob', 'Engineering', 88000, 33, 'Seattle'),\n",
       " (3, 'Carol', 'HR', 72000, 41, 'New York'),\n",
       " (4, 'David', 'Finance', 85000, 38, 'Chicago'),\n",
       " (5, 'Eve', 'Engineering', 99000, 27, 'Austin'),\n",
       " (6, 'Frank', 'HR', 68000, 45, 'Boston'),\n",
       " (7, 'Grace', 'Finance', 79000, 31, 'New York'),\n",
       " (8, 'Heidi', 'Engineering', 92000, 35, 'Seattle'),\n",
       " (9, 'Ivan', 'Marketing', 66000, 30, 'Chicago'),\n",
       " (10, 'Judy', 'Marketing', 71000, 28, 'Austin'),\n",
       " (11, 'Mallory', 'Finance', 81000, 42, 'Boston'),\n",
       " (12, 'Oscar', 'HR', 70000, 36, 'Seattle')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sc = spark.sparkContext\n",
    "rdd_lines = sc.textFile(\"employees.csv\")\n",
    "header = rdd_lines.first()\n",
    "data_rdd = rdd_lines.filter(lambda line: line != header)\n",
    "\n",
    "def parse(line):\n",
    "    parts = line.split(\",\")\n",
    "    return (int(parts[0]), parts[1], parts[2], int(parts[3]), int(parts[4]), parts[5])\n",
    "\n",
    "employees_rdd = data_rdd.map(parse)\n",
    "all_employees = employees_rdd.collect()\n",
    "print(employees_rdd.toDebugString()) # will print the parent RDDs and partitioning info.\n",
    "all_employees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e9778cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374000\n"
     ]
    }
   ],
   "source": [
    "eng_salaries_rdd = employees_rdd.filter(lambda t: t[2] == \"Engineering\").map(lambda t: t[3])\n",
    "total_eng_salary = eng_salaries_rdd.sum()\n",
    "print(total_eng_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60e75f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/developer/Workspace_Projects/Data_Engineer/PySpark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a18f93b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Engineering', 374000)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "r1 = sc.textFile(\"employees.csv\")\n",
    "r2 = r1.filter(lambda line: \"Engineering\" in line)\n",
    "r3 = r2.map(parse)                      \n",
    "r4 = r3.map(lambda t: (t[2], int(t[3]))) \n",
    "r5 = r4.reduceByKey(lambda a,b: a+b)         \n",
    "result = r5.collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bc9ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 95000), ('Bob', 88000), ('Eve', 99000)]\n",
      "Partitions: 200\n"
     ]
    }
   ],
   "source": [
    "#  parallelize() → creates an RDD of type ParallelCollectionRDD\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "data = [(\"Alice\", 95000), (\"Bob\", 88000), (\"Eve\", 99000)]\n",
    "rdd = sc.parallelize(data, numSlices=200)\n",
    "\n",
    "print(rdd.collect())\n",
    "print(\"Partitions:\", rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc2dced7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:====================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 100000), ('Bob', 93000), ('Eve', 104000)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x: (x[0], x[1] + 5000))\n",
    "print(rdd2.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb774bc",
   "metadata": {},
   "source": [
    "- You define transformations → Spark creates a logical plan (lineage graph).\n",
    "\n",
    "- No work happens yet — just metadata.\n",
    " \n",
    "- You trigger an action → Spark’s DAG Scheduler:\n",
    " \n",
    "- Divides operations into stages.\n",
    " \n",
    "- Launches tasks on executors.\n",
    " \n",
    "- Performs narrow and wide shuffles if needed.\n",
    " \n",
    "- Result is returned to driver or saved to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d0d2e",
   "metadata": {},
   "source": [
    "| Type      | Description                                                                       | Examples                      |\n",
    "|-----------|-----------------------------------------------------------------------------------|--------                       |\n",
    "|`Narrow`\t|Each partition of parent RDD is used by at most one child partition (no shuffle).\t|map, filter, flatMap\n",
    "|`Wide`\t    |Data from multiple parent partitions needed for one child partition → causes shuffle.\t|reduceByKey, groupByKey, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "91b9d537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'Aisha', 'Math', '90'],\n",
       " ['2', 'Raj', 'Science', '80'],\n",
       " ['3', 'Neha', 'Math', '85'],\n",
       " ['4', 'Raj', 'Math', '70'],\n",
       " ['5', 'Aisha', 'English', '88']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rdd = sc.textFile(\"students.csv\")\n",
    "\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda line: line != header)\n",
    "\n",
    "parsed_rdd = data_rdd.map(lambda line: line.split(\",\"))\n",
    "parsed_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6609492",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "98c95caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aisha', 178), ('Raj', 150), ('Neha', 85)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd = parsed_rdd.map(lambda x: (x[1], int(x[3])))\n",
    "total_marks = pair_rdd.reduceByKey(lambda a, b: a+b)\n",
    "total_marks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "52aaf818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aisha', [90, 88]), ('Raj', [80, 70]), ('Neha', [85])]\n"
     ]
    }
   ],
   "source": [
    "grouped_marks = pair_rdd.groupByKey()\n",
    "print([(name, list(marks)) for name, marks in grouped_marks.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6910891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Math', 'Science', 'Math', 'Math', 'English']\n"
     ]
    }
   ],
   "source": [
    "# flatMap(func) is like map(), but each input item can be mapped to 0 or more output items.\n",
    "\n",
    "subjects_rdd = parsed_rdd.map(lambda x: x[2])  # ['Math', 'Science', 'Math', 'Math', 'English']\n",
    "flat_rdd = subjects_rdd.flatMap(lambda x: x.split(\",\"))\n",
    "print(flat_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d4a05b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Raj', (80, 'Mumbai')), ('Raj', (70, 'Mumbai')), ('Aisha', (90, 'Delhi')), ('Aisha', (88, 'Delhi')), ('Neha', (85, 'Pune'))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "city_rdd = sc.parallelize([(\"Aisha\", \"Delhi\"), (\"Raj\", \"Mumbai\"), (\"Neha\", \"Pune\")])\n",
    "\n",
    "joined = pair_rdd.join(city_rdd)\n",
    "print(joined.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38f722a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 5\n"
     ]
    }
   ],
   "source": [
    "count = parsed_rdd.count()\n",
    "print(\"Total records:\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "94d0e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'Aisha', 'Math', '90'], ['2', 'Raj', 'Science', '80']]\n"
     ]
    }
   ],
   "source": [
    "first_two = parsed_rdd.take(2)\n",
    "print(first_two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1cdbe7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total marks: 413\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = parsed_rdd.map(lambda x: int(x[3]))\n",
    "total_marks = marks_rdd.reduce(lambda a, b: a + b)\n",
    "print(\"Total marks:\", total_marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702311b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_rdd = parsed_rdd.map(lambda x: (x[1], int(x[3])))\n",
    "pair_rdd.saveAsTextFile(\"output/student_marks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2f06ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Student: Neha, Marks: 85\n",
      "Student: Raj, Marks: 70\n",
      "Student: Aisha, Marks: 88\n",
      "Student: Aisha, Marks: 90\n",
      "Student: Raj, Marks: 80\n"
     ]
    }
   ],
   "source": [
    "pair_rdd.foreach(lambda x: print(f\"Student: {x[0]}, Marks: {x[1]}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59669e",
   "metadata": {},
   "source": [
    "What is a **Lineage Graph**?\n",
    "\n",
    "Every RDD in Spark remembers how it was derived from other RDDs.\n",
    "\n",
    "This chain of dependencies between RDDs forms a **Lineage Graph** (also called DAG — Directed Acyclic Graph).\n",
    "\n",
    "➡️ Instead of storing every intermediate dataset, Spark keeps:\n",
    "\n",
    "-The sequence of transformations (map, filter, reduceByKey, etc.)\n",
    "\n",
    "-The original data source path\n",
    "\n",
    "-The functions applied\n",
    "\n",
    "So, if a partition of data is lost (say, due to node failure), Spark can recompute it by replaying the transformations from the original source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "df4b52e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without cache: 0.24838852882385254\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for _ in range(3):\n",
    "    parsed_rdd.filter(lambda x: int(x[3]) > 80).count()\n",
    "print(\"Without cache:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "56612bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With cache: 0.19412803649902344\n"
     ]
    }
   ],
   "source": [
    "cached_rdd = parsed_rdd.filter(lambda x: int(x[3]) > 80).cache()\n",
    "cached_rdd.count()  # first computation (cached)\n",
    "start = time.time()\n",
    "for _ in range(3):\n",
    "    cached_rdd.count()\n",
    "print(\"With cache:\", time.time() - start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
